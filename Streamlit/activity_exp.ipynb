{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "nltk.download('wordnet')          \n",
    "nltk.download('stopwords')       \n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('activity_table.pkl', 'rb') as f:\n",
    "    activity_table = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting activities to clean and prepare for clustering\n",
    "activities_to_clean = set(activity_table['activity_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing empty actvities\n",
    "activities_to_clean = {activity for activity in activities_to_clean if activity.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing words in ()\n",
    "def remove_words_in_parentheses(activity):\n",
    "    return re.sub(r'\\([^)]*\\)', '', activity)\n",
    "activities_to_clean = {remove_words_in_parentheses(activity) for activity in activities_to_clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting activities and keep everything before '/'\n",
    "cleaned_activities = set()\n",
    "for activity in activities_to_clean:\n",
    "    if '/' in activity:\n",
    "        cleaned_activity = activity.split('/')[0].strip()\n",
    "    else:\n",
    "        cleaned_activity = activity.strip()\n",
    "    cleaned_activities.add(cleaned_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to lowercase\n",
    "cleaned_activities = {activity.lower() for activity in cleaned_activities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmitizing activities\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))  # assuming English stopwords\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text.lower())  # lowercase tokens\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a cleaned text string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "cleaned_texts = [lemmatize_text(text) for text in cleaned_activities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding anagrams and keeping one instance of anagrams\n",
    "def keep_one_anagram(activities):\n",
    "    anagram_map = defaultdict(list)\n",
    "\n",
    "    # Create a dictionary where keys are sorted activities and values are the original activities\n",
    "    for activity in activities:\n",
    "        sorted_activity = ' '.join(sorted(activity.split()))\n",
    "        anagram_map[sorted_activity].append(activity)\n",
    "\n",
    "    # List to store unique activities (one from each set of anagrams)\n",
    "    unique_activities = []\n",
    "\n",
    "    # Keep one instance of each set of anagrams\n",
    "    for sorted_activity, orig_activities in anagram_map.items():\n",
    "        unique_activities.append(orig_activities[0])  \n",
    "\n",
    "    return unique_activities\n",
    "clean_activities = keep_one_anagram(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_activities.json', 'w') as file:\n",
    "    json.dump(clean_activities, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"openai_key_here\"\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API call for clustering\n",
    "def industry_cluster_extractor(content):\n",
    "    role = \"\"\"You are given a list of business areas or industries.\n",
    "\n",
    "            Generate names for clusters that the business areas or industries belong to.\n",
    "\n",
    "            Cluster based on the semantic meaning of the business areas or industries. \n",
    "\n",
    "            Output a list of semantically coherent and distinct cluster names.\n",
    "\n",
    "            Do not concentrate on the operational model (i.e., import, export, manufacturing, etc.).\n",
    "\n",
    "            Concentrate on the sector/industry of the business activity.\n",
    "\n",
    "            Do not add bullet points, numbering, or any other text formatting.\n",
    "            \"\"\"\n",
    "    \n",
    "\n",
    "    chat_completion = client.chat.completions.create(model = \"gpt-4o\",\n",
    "                                                     messages = [{\"role\": \"system\", \"content\": role},\n",
    "                                                                 {\"role\": \"user\", \"content\": content}])\n",
    "\n",
    "    output = chat_completion.choices[0].message.content\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_str = \"\\n\\n\".join(clean_activities)\n",
    "industry_clusters = industry_cluster_extractor(industry_str)\n",
    "industry_cluster_names = [industry.strip() for industry in industry_clusters.replace('- ', '').splitlines()]\n",
    "api_clusters = {item for item in industry_cluster_names if item}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_clusters = list(api_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_clusters = ['Education and Training Services',\n",
    "'Healthcare and Medical Services',\n",
    "'Manufacturing (General)',\n",
    "'Agriculture and Farming',\n",
    "'Retail and E-commerce',\n",
    "'Food and Beverage Production',\n",
    "'Hospitality and Tourism',\n",
    "'Automotive and Transportation Services',\n",
    "'Construction and Real Estate',\n",
    "'Technology and IT Services',\n",
    "'Energy and Utilities',\n",
    "'Financial Services',\n",
    "'Arts, Crafts, and Entertainment',\n",
    "'Environmental Services',\n",
    "'Consulting and Business Services',\n",
    "'Logistics and Supply Chain',\n",
    "'Legal and Security Services',\n",
    "'Media and Communication',\n",
    "'Social and Community Services',\n",
    "'Beauty and Personal Care Services',\n",
    "'Cleaning and Maintenance Services',\n",
    "'Sports and Recreation',\n",
    "'Government and Public Services']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_activities = set(activity_table['activity_name'])\n",
    "activities = list(unique_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API call for labeling\n",
    "def industry_clusterer(activities, clusters): \n",
    "    role = f\"\"\"You are given a list of business areas or industry clusters below. \n",
    "    {clusters}\n",
    "    I will give you a single list of business areas or industry. Determine which of the business areas or industry clusters the given business areas or industry belongs to.\n",
    "    Do not return any text other than cluster. Do not leave any labels empty and do not add any additional text of code formatting markers.\n",
    "    \"\"\"\n",
    "    content = f'{activities}'\n",
    "        \n",
    "    chat_completion =client.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [{\"role\": \"system\", \"content\": role},\n",
    "                    {\"role\": \"user\", \"content\": content}]\n",
    "    )\n",
    "    output = chat_completion.choices[0].message.content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_activities = {}\n",
    "def label_activities(activities, clusters):\n",
    "    global labeled_activities = {}\n",
    "    for activity in activities:\n",
    "        cluster = industry_clusterer(activity, clusters)\n",
    "        labeled_activities[activity] = cluster\n",
    "        print(f'{activity} added to dict as {cluster}')\n",
    "    \n",
    "    print(\"Labeling process completed.\")\n",
    "    print(f\"Total activities labeled: {len(labeled_activities)}\")\n",
    "    return labeled_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_activities = label_activities(activities, act_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labeled_act.json', 'w') as json_file:   \n",
    "    json.dump(labeled_activities, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labeled_act.json', 'r') as file:\n",
    "    labeled_activities = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_table['cluster'] = activity_table['activity_name'].map(labeled_activities) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_table.to_pickle('activity_table.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
