{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')          \n",
    "nltk.download('stopwords')       \n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('product_table.pkl', 'rb') as f:\n",
    " product_table = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_to_clean = set(product_table['product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing empty products\n",
    "products_to_clean = {product for product in products_to_clean if product.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing words in ()\n",
    "def remove_words_in_parentheses(product):\n",
    "    return re.sub(r'\\([^)]*\\)', '', product)\n",
    "products_to_clean = {remove_words_in_parentheses(product) for product in products_to_clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting activities and keep everything before '/'\n",
    "cleaned_products = set()\n",
    "for product in products_to_clean:\n",
    "    if '/' in product:\n",
    "        cleaned_product = product.split('/')[0].strip()\n",
    "    else:\n",
    "        cleaned_product = product.strip()\n",
    "    cleaned_products.add(cleaned_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to lowercase\n",
    "cleaned_products = {product.lower() for product in cleaned_products}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmitizing activities\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))  # assuming English stopwords\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text.lower())  # lowercase tokens\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a cleaned text string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "cleaned_texts = [lemmatize_text(text) for text in cleaned_products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_products.json', 'w') as file:\n",
    "    json.dump(cleaned_texts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean_products.json', 'r') as file:\n",
    "    cleaned_texts = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_cluster = [item for item in cleaned_texts if len(item) <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"openai_key_here\"\n",
    "client = OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API call for clustering\n",
    "def industry_cluster_extractor(content):\n",
    "    role = \"\"\"You are given a list of business areas or industries.\n",
    "\n",
    "            Generate names for clusters that the business areas or industries belong to.\n",
    "\n",
    "            Cluster based on the semantic meaning of the business areas or industries. \n",
    "\n",
    "            Output a list of semantically coherent and distinct cluster names.\n",
    "\n",
    "            Do not concentrate on the operational model (i.e., import, export, manufacturing, etc.).\n",
    "\n",
    "            Concentrate on the sector/industry of the business activity.\n",
    "\n",
    "            Do not add bullet points, numbering, or any other text formatting.\n",
    "            \"\"\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(model = \"gpt-4o\",\n",
    "                                                     messages = [{\"role\": \"system\", \"content\": role},\n",
    "                                                                 {\"role\": \"user\", \"content\": content}])\n",
    "\n",
    "    output = chat_completion.choices[0].message.content\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_str = \"\\n\\n\".join(texts_to_cluster)\n",
    "industry_clusters = industry_cluster_extractor(industry_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_cluster_names = [industry.strip() for industry in industry_clusters.replace('- ', '').splitlines()]\n",
    "api_clusters = {item for item in industry_cluster_names if item}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_clusters = list(api_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_clusters = ['Transportation and Logistics',\n",
    "'Construction and Infrastructure',\n",
    "'Hospitality and Tourism',\n",
    "'Food and Beverage',\n",
    "'Technology and IT Services',\n",
    "'Healthcare and Medical Equipment',\n",
    "'Legal and Consulting Services',\n",
    "'Arts and Crafts',\n",
    "'Education and Training',\n",
    "'Real Estate and Property Management',\n",
    "'Retail and Consumer Goods',\n",
    "'Manufacturing and Industrial Equipment',\n",
    "'Financial and Insurance Services',\n",
    "'Marketing and Advertising',\n",
    "'Personal Care and Beauty Services',\n",
    "'Agriculture and Farming',\n",
    "'Environmental and Safety Services',\n",
    "'Furniture and Interior Design',\n",
    "'Energy and Utilities',\n",
    "'Entertainment and Event Management',\n",
    "'Engineering and Technical Services',\n",
    "'Fashion and Apparel',\n",
    "'Printing and Publishing',\n",
    "'Childcare and Educational Services']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = set(product_table['product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = list(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('product_list.json', 'w') as file:\n",
    "    json.dump(product_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API call for labeling\n",
    "def industry_clusterer(products, clusters): \n",
    "    role = f\"\"\"You are given a list of business areas or industry clusters below. \n",
    "    {clusters}\n",
    "    I will give you a single list of business areas or industry. Determine which of the business areas or industry clusters the given business areas or industry belongs to.\n",
    "    Do not return any text other than cluster. Do not leave any labels empty and do not add any additional text of code formatting markers.\n",
    "    \"\"\"\n",
    "        \n",
    "    chat_completion =client.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [{\"role\": \"system\", \"content\": role},\n",
    "                    {\"role\": \"user\", \"content\": products}]\n",
    "    )\n",
    "    output = chat_completion.choices[0].message.content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_products = {}\n",
    "def label_products(products, clusters):\n",
    "    global labeled_products = {}\n",
    "    for product in products:\n",
    "        cluster = industry_clusterer(product, clusters)\n",
    "        labeled_products[product] = cluster\n",
    "        print(f'{product} added to dict as {cluster}')\n",
    "    \n",
    "    print(\"Labeling process completed.\")\n",
    "    print(f\"Total activities labeled: {len(labeled_products)}\")\n",
    "    return labeled_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labeled_products_final.json', 'r') as file:\n",
    "    labeled_products = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_table['label'] = product_table['product_name'].map(labeled_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_table.to_pickle('product_table.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
